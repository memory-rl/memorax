# Algorithm specific arguments
name: ppo
learning_rate: 5e-3
num_envs: 64
num_eval_envs: 64
num_steps: 128
anneal_lr: False
gamma: 0.99
gae_lambda: 0.95
num_minibatches: 8
update_epochs: 8
batch_size: ${eval:${.num_envs}*${.num_steps}}
normalize_advantage: True
clip_coef: 0.2
clip_vloss: False
ent_coef: 0.0
vf_coef: 0.5
max_grad_norm: 0.5
learning_starts: 0

actor:
  feature_extractor:
    _target_: memory_rl.networks.MLP
    features: [64]
    kernel_init: 
      _target_: flax.linen.initializers.orthogonal
      scale: 1.414
  torso:
    _target_: memory_rl.networks.MLP
    features: [64]
    kernel_init: 
      _target_: flax.linen.initializers.orthogonal
      scale: 1.414

critic:
  feature_extractor:
    _target_: memory_rl.networks.MLP
    features: [64]
    kernel_init: 
      _target_: flax.linen.initializers.orthogonal
      scale: 1.414
  torso:
    _target_: memory_rl.networks.MLP
    features: [64]
    kernel_init:
      _target_: flax.linen.initializers.orthogonal
      scale: 1.414
