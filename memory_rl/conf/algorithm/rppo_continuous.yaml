# Algorithm specific arguments
name: rppo_continuous
learning_rate: 0.0003
num_envs: 512
num_eval_envs: 10
num_steps: 16
anneal_lr: False
gamma: 0.99
gae_lambda: 0.95
num_minibatches: 128
update_epochs: 5
norm_adv: False
clip_coef: 0.2
clip_vloss: False
ent_coef: 0.01
vf_coef: 0.5
max_grad_norm: 0.5
learning_starts: 0
hidden_dims: [128]
actor_cell_size: 128
critic_cell_size: 128
# Additional parameters for continuous action spaces
max_action: 1.0
policy_log_std_min: -10
policy_log_std_max: 2
policy_final_fc_init_scale: 1e-3
