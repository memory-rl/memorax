# @package _global_

algorithm:
  _target_: memorax.algorithms.PPOConfig
  name: ppo
  num_envs: 32
  num_eval_envs: 16
  num_steps: 128
  gamma: 0.95
  gae_lambda: 0.95
  num_minibatches: 4
  update_epochs: 4
  normalize_advantage: True
  clip_coef: 0.2
  clip_vloss: True
  ent_coef: 0.01
  vf_coef: 0.5
