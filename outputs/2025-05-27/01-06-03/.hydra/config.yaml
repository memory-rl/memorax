total_timesteps: 1000000
num_train_steps: 100000
num_evaluation_steps: 20000
seed: 0
num_seeds: 1
environment:
  env_id: ant
algorithm:
  name: rsac
  policy_lr: 0.0003
  q_lr: 0.0003
  temp_lr: 0.0003
  num_envs: 1
  num_eval_envs: 1
  buffer_size: 1000000
  gamma: 0.99
  tau: 0.005
  train_frequency: 1
  target_update_frequency: 1
  batch_size: 256
  hidden_dims:
  - 256
  - 256
  actor_cell_size: 256
  critic_cell_size: 256
  sample_sequence_length: 8
  init_temperature: 1.0
  policy_log_std_min: -5
  policy_log_std_max: 2
  policy_final_fc_init_scale: 0.001
  target_entropy_multiplier: 1.0
  max_action: 1.0
  backup_entropy: true
  learning_starts: 5000
logger:
  track: false
