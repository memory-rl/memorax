# Algorithm specific arguments
name: rppo_continuous
learning_rate: 0.0003
num_envs: 512
num_eval_envs: 10
num_steps: 16
anneal_lr: False
gamma: 0.99
gae_lambda: 0.95
num_minibatches: 128
update_epochs: 5
norm_adv: False
clip_coef: 0.2
clip_vloss: False
ent_coef: 0.01
vf_coef: 0.5
max_grad_norm: 0.5
learning_starts: 0

max_action: 1.0
policy_log_std_min: -10
policy_log_std_max: 2
policy_final_fc_init_scale: 1e-3

actor:
  feature_extractor:
    _target_: memory_rl.networks.feature_extractors.MLP
    features: [128]
  cell:
    _target_: memory_rl.networks.recurrent.MaskedGRUCell
    features: 128
critic:
  feature_extractor:
    _target_: memory_rl.networks.feature_extractors.MLP
    features: [128]
  cell:
    _target_: memory_rl.networks.recurrent.MaskedGRUCell
    features: 128
